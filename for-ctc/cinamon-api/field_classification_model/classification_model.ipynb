{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import random, io\n",
    "from MeCab import Tagger\n",
    "import numpy as np\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import initializers, constraints, layers, callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Input, BatchNormalization, LSTM, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Add\n",
    "from keras.layers import GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel(excel_file):\n",
    "    sentences_sum = []\n",
    "    sentences_tax = []\n",
    "    sentences_total = []\n",
    "\n",
    "    df = pd.read_excel(excel_file, sheet_name=0, keep_default_na=False)\n",
    "    for row in range(0, df.shape[0]):\n",
    "\n",
    "        # read and normalize unicode\n",
    "        content_of_sum =  unicodedata.normalize('NFKC', df.loc[row, \"sum detail of account \"].lower())\n",
    "        content_of_tax = unicodedata.normalize('NFKC', df.loc[row, \"tax\"].lower())\n",
    "        content_of_total = unicodedata.normalize('NFKC', df.loc[row, \"total amount \"].lower())\n",
    "\n",
    "        # replace space, ()〈〉 and split string\n",
    "        sentences_sum.extend(re.sub('[ ]', '', content_of_sum).split('\\n'))\n",
    "        sentences_tax.extend(re.sub('[ ]', '', content_of_tax).split('\\n'))\n",
    "        sentences_total.extend(re.sub('[ ]', '', content_of_total).split('\\n'))\n",
    "\n",
    "    # Remove null\n",
    "    sentences_sum = [phrase for phrase in sentences_sum if phrase]\n",
    "    sentences_tax = [phrase for phrase in sentences_tax if phrase]\n",
    "    sentences_total = [phrase for phrase in sentences_total if phrase]\n",
    "\n",
    "    # get unique\n",
    "    sentences_sum = list(set(sentences_sum))\n",
    "    sentences_tax = list(set(sentences_tax))\n",
    "    sentences_total = list(set(sentences_total))\n",
    "\n",
    "    data = []\n",
    "\n",
    "    data.extend([phrase, '1'] for phrase in sentences_sum)\n",
    "    data.extend([phrase, '2'] for phrase in sentences_tax)\n",
    "    data.extend([phrase, '3'] for phrase in sentences_total)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(Tagger):\n",
    "    word_index = {}\n",
    "    max_len = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        Tagger.__init__(self)\n",
    "\n",
    "    def tokenize(self, sequence):\n",
    "        '''Tokenize a string using mecab'''\n",
    "        sequence = sequence.replace(' ', '')\n",
    "        sequence = ''.join([unicodedata.normalize('NFKC', char) for char in sequence])\n",
    "        sequence = self.parse(sequence).splitlines()\n",
    "        sequence = [line.split('\\t')[0] for line in sequence]\n",
    "        sequence = [token for token in sequence if token != 'EOS']\n",
    "        sequence = [list(token) if token.isnumeric() else [token] for token in sequence]\n",
    "        sequence = [token for sublist in sequence for token in sublist]\n",
    "        return sequence\n",
    "\n",
    "    def fit_on_texts(self, sentences):\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            all_tokens.extend(tokens)\n",
    "            if len(tokens) > self.max_len:\n",
    "                self.max_len = len(tokens)\n",
    "        unique_tokens = np.unique(all_tokens)\n",
    "\n",
    "        for i, token in enumerate(unique_tokens):\n",
    "            self.word_index[token] = i + 1\n",
    "\n",
    "    def texts_to_sequences(self, sentences):\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            tokenized = []\n",
    "            for token in tokens:\n",
    "                tokenized.append(self.word_index[token])\n",
    "            all_tokens.append(tokenized)\n",
    "        return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_index, word_index, max_features, embed_size=300):\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_other_class(list_words, max_len, number_sample, class_name):\n",
    "    data = []\n",
    "    list_words = np.array(list_words)\n",
    "    for i in range(0, number_sample):\n",
    "        random_size = random.randint(1, max_len)\n",
    "        indices = np.random.randint(len(list_words), size=random_size)\n",
    "        tokens = list_words[indices]\n",
    "        sentences = ''.join(tokens)\n",
    "        data.append([sentences, class_name])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = '/Users/anh/Downloads/listkey.xlsx'\n",
    "fasttext_file = '/Users/anh/Downloads/cc.ja.300.vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data and preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = read_excel(excel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load fasttext vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = load_vectors(fasttext_file)\n",
    "list_words = list(vector.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create data for class 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "num_sample = 50\n",
    "data_other_class = create_data_for_other_class(list_words, max_len, num_sample, '4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((excel_data, data_other_class), axis=0)\n",
    "X_data = data[:, :-1]\n",
    "y = data[:, -1:].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X_data.flatten()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenized = tokenizer.texts_to_sequences(sentences)\n",
    "X_train = pad_sequences(tokenized, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)\n",
    "embedding_matrix = build_embedding_matrix(vector, word_index, max_features, embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert y to one hot vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 20, 300)           153000    \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 20, 64)            57664     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_24 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 210,924\n",
      "Trainable params: 57,924\n",
      "Non-trainable params: 153,000\n",
      "_________________________________________________________________\n",
      "Train on 185 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      " 32/185 [====>.........................] - ETA: 6s - loss: 0.7478 - acc: 0.3438Epoch 00000: val_loss improved from inf to 0.63883, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 2s - loss: 0.6342 - acc: 0.6797 - val_loss: 0.6388 - val_acc: 0.7500\n",
      "Epoch 2/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.5019 - acc: 0.8203Epoch 00001: val_loss improved from 0.63883 to 0.57013, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.4487 - acc: 0.8581 - val_loss: 0.5701 - val_acc: 0.7976\n",
      "Epoch 3/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.3724 - acc: 0.8672Epoch 00002: val_loss improved from 0.57013 to 0.52399, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3626 - acc: 0.8797 - val_loss: 0.5240 - val_acc: 0.8095\n",
      "Epoch 4/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.3102 - acc: 0.9016Epoch 00003: val_loss improved from 0.52399 to 0.48405, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3058 - acc: 0.9095 - val_loss: 0.4841 - val_acc: 0.8452\n",
      "Epoch 5/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2727 - acc: 0.9266Epoch 00004: val_loss improved from 0.48405 to 0.44255, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2676 - acc: 0.9284 - val_loss: 0.4426 - val_acc: 0.8810\n",
      "Epoch 6/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2403 - acc: 0.9406Epoch 00005: val_loss improved from 0.44255 to 0.40532, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2395 - acc: 0.9419 - val_loss: 0.4053 - val_acc: 0.8929\n",
      "Epoch 7/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2286 - acc: 0.9469Epoch 00006: val_loss improved from 0.40532 to 0.37608, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2187 - acc: 0.9486 - val_loss: 0.3761 - val_acc: 0.9048\n",
      "Epoch 8/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.2454 - acc: 0.9453Epoch 00007: val_loss improved from 0.37608 to 0.35235, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2015 - acc: 0.9500 - val_loss: 0.3523 - val_acc: 0.9405\n",
      "Epoch 9/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1658 - acc: 0.9688Epoch 00008: val_loss improved from 0.35235 to 0.33652, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1880 - acc: 0.9541 - val_loss: 0.3365 - val_acc: 0.9643\n",
      "Epoch 10/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1181 - acc: 0.9688Epoch 00009: val_loss improved from 0.33652 to 0.32236, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1771 - acc: 0.9568 - val_loss: 0.3224 - val_acc: 0.9643\n",
      "Epoch 11/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1634 - acc: 0.9609Epoch 00010: val_loss improved from 0.32236 to 0.30960, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1666 - acc: 0.9595 - val_loss: 0.3096 - val_acc: 0.9762\n",
      "Epoch 12/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1530 - acc: 0.9641Epoch 00011: val_loss improved from 0.30960 to 0.29682, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1581 - acc: 0.9595 - val_loss: 0.2968 - val_acc: 0.9762\n",
      "Epoch 13/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.2098 - acc: 0.9375Epoch 00012: val_loss improved from 0.29682 to 0.28758, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1511 - acc: 0.9608 - val_loss: 0.2876 - val_acc: 0.9881\n",
      "Epoch 14/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1525 - acc: 0.9766Epoch 00013: val_loss improved from 0.28758 to 0.27712, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1440 - acc: 0.9622 - val_loss: 0.2771 - val_acc: 0.9881\n",
      "Epoch 15/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1614 - acc: 0.9609Epoch 00014: val_loss improved from 0.27712 to 0.27003, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1376 - acc: 0.9662 - val_loss: 0.2700 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1268 - acc: 0.9734Epoch 00015: val_loss improved from 0.27003 to 0.26201, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1315 - acc: 0.9689 - val_loss: 0.2620 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1268 - acc: 0.9703Epoch 00016: val_loss improved from 0.26201 to 0.25421, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1266 - acc: 0.9716 - val_loss: 0.2542 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1246 - acc: 0.9688Epoch 00017: val_loss improved from 0.25421 to 0.24696, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1221 - acc: 0.9703 - val_loss: 0.2470 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0967 - acc: 0.9844Epoch 00018: val_loss improved from 0.24696 to 0.24056, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1172 - acc: 0.9716 - val_loss: 0.2406 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1127 - acc: 0.9734Epoch 00019: val_loss improved from 0.24056 to 0.23411, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1131 - acc: 0.9716 - val_loss: 0.2341 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0975 - acc: 0.9734Epoch 00020: val_loss improved from 0.23411 to 0.22803, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1093 - acc: 0.9730 - val_loss: 0.2280 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1003 - acc: 0.9844Epoch 00021: val_loss improved from 0.22803 to 0.22151, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1068 - acc: 0.9743 - val_loss: 0.2215 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1083 - acc: 0.9844Epoch 00022: val_loss improved from 0.22151 to 0.21602, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1030 - acc: 0.9770 - val_loss: 0.2160 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1011 - acc: 0.9750Epoch 00023: val_loss improved from 0.21602 to 0.21030, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1000 - acc: 0.9757 - val_loss: 0.2103 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0762 - acc: 0.9844Epoch 00024: val_loss improved from 0.21030 to 0.20503, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0968 - acc: 0.9797 - val_loss: 0.2050 - val_acc: 1.0000\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0984 - acc: 0.9797Epoch 00025: val_loss improved from 0.20503 to 0.20013, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0940 - acc: 0.9797 - val_loss: 0.2001 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0720 - acc: 1.0000Epoch 00026: val_loss improved from 0.20013 to 0.19503, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0915 - acc: 0.9797 - val_loss: 0.1950 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0784 - acc: 0.9844Epoch 00027: val_loss improved from 0.19503 to 0.19029, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0894 - acc: 0.9797 - val_loss: 0.1903 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1007 - acc: 0.9844Epoch 00028: val_loss improved from 0.19029 to 0.18600, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0865 - acc: 0.9811 - val_loss: 0.1860 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0533 - acc: 0.9844Epoch 00029: val_loss improved from 0.18600 to 0.18146, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0855 - acc: 0.9784 - val_loss: 0.1815 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0852 - acc: 0.9766Epoch 00030: val_loss improved from 0.18146 to 0.17739, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0827 - acc: 0.9784 - val_loss: 0.1774 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1087 - acc: 0.9844Epoch 00031: val_loss improved from 0.17739 to 0.17316, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0819 - acc: 0.9784 - val_loss: 0.1732 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0696 - acc: 0.9844Epoch 00032: val_loss improved from 0.17316 to 0.16913, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0795 - acc: 0.9797 - val_loss: 0.1691 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1184 - acc: 0.9375Epoch 00033: val_loss improved from 0.16913 to 0.16506, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0790 - acc: 0.9770 - val_loss: 0.1651 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0844 - acc: 0.9750Epoch 00034: val_loss improved from 0.16506 to 0.16074, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0775 - acc: 0.9784 - val_loss: 0.1607 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0668 - acc: 1.0000Epoch 00035: val_loss improved from 0.16074 to 0.15712, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0744 - acc: 0.9811 - val_loss: 0.1571 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0470 - acc: 1.0000Epoch 00036: val_loss improved from 0.15712 to 0.15391, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0723 - acc: 0.9797 - val_loss: 0.1539 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1065 - acc: 0.9531Epoch 00037: val_loss improved from 0.15391 to 0.15053, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0726 - acc: 0.9811 - val_loss: 0.1505 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0620 - acc: 1.0000Epoch 00038: val_loss improved from 0.15053 to 0.14727, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0700 - acc: 0.9811 - val_loss: 0.1473 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0666 - acc: 0.9812Epoch 00039: val_loss improved from 0.14727 to 0.14389, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0685 - acc: 0.9811 - val_loss: 0.1439 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1199 - acc: 0.9531Epoch 00040: val_loss improved from 0.14389 to 0.14057, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0667 - acc: 0.9811 - val_loss: 0.1406 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0671 - acc: 0.9688Epoch 00041: val_loss improved from 0.14057 to 0.13763, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0660 - acc: 0.9811 - val_loss: 0.1376 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0542 - acc: 0.9844Epoch 00042: val_loss improved from 0.13763 to 0.13466, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0659 - acc: 0.9784 - val_loss: 0.1347 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0747 - acc: 0.9688Epoch 00043: val_loss improved from 0.13466 to 0.13198, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0629 - acc: 0.9811 - val_loss: 0.1320 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0500 - acc: 0.9844Epoch 00044: val_loss improved from 0.13198 to 0.12904, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0626 - acc: 0.9811 - val_loss: 0.1290 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0592 - acc: 0.9781Epoch 00045: val_loss improved from 0.12904 to 0.12634, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0617 - acc: 0.9784 - val_loss: 0.1263 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0325 - acc: 1.0000Epoch 00046: val_loss improved from 0.12634 to 0.12369, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0603 - acc: 0.9811 - val_loss: 0.1237 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0675 - acc: 0.9844Epoch 00047: val_loss improved from 0.12369 to 0.12086, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0599 - acc: 0.9811 - val_loss: 0.1209 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0217 - acc: 1.0000Epoch 00048: val_loss improved from 0.12086 to 0.11815, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0585 - acc: 0.9811 - val_loss: 0.1181 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0793 - acc: 0.9844Epoch 00049: val_loss improved from 0.11815 to 0.11573, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0587 - acc: 0.9811 - val_loss: 0.1157 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0419 - acc: 0.9844Epoch 00050: val_loss improved from 0.11573 to 0.11349, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0560 - acc: 0.9811 - val_loss: 0.1135 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0724 - acc: 0.9844Epoch 00051: val_loss improved from 0.11349 to 0.11079, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0565 - acc: 0.9811 - val_loss: 0.1108 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0510 - acc: 0.9781Epoch 00052: val_loss improved from 0.11079 to 0.10855, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0550 - acc: 0.9784 - val_loss: 0.1086 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0558 - acc: 0.9844Epoch 00053: val_loss improved from 0.10855 to 0.10653, saving model to Model1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 0s - loss: 0.0537 - acc: 0.9811 - val_loss: 0.1065 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0734 - acc: 0.9688Epoch 00054: val_loss improved from 0.10653 to 0.10459, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0536 - acc: 0.9811 - val_loss: 0.1046 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0577 - acc: 0.9781Epoch 00055: val_loss improved from 0.10459 to 0.10250, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0527 - acc: 0.9811 - val_loss: 0.1025 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0568 - acc: 0.9781Epoch 00056: val_loss improved from 0.10250 to 0.10046, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0521 - acc: 0.9811 - val_loss: 0.1005 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0178 - acc: 1.0000Epoch 00057: val_loss improved from 0.10046 to 0.09858, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0512 - acc: 0.9784 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0883 - acc: 0.9531Epoch 00058: val_loss improved from 0.09858 to 0.09677, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0507 - acc: 0.9811 - val_loss: 0.0968 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0433 - acc: 0.9844Epoch 00059: val_loss improved from 0.09677 to 0.09494, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0503 - acc: 0.9811 - val_loss: 0.0949 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0815 - acc: 0.9688Epoch 00060: val_loss improved from 0.09494 to 0.09306, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0498 - acc: 0.9811 - val_loss: 0.0931 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0547 - acc: 0.9734Epoch 00061: val_loss improved from 0.09306 to 0.09132, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0517 - acc: 0.9770 - val_loss: 0.0913 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0464 - acc: 0.9922Epoch 00062: val_loss improved from 0.09132 to 0.08975, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0486 - acc: 0.9824 - val_loss: 0.0898 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0576 - acc: 0.9844Epoch 00063: val_loss improved from 0.08975 to 0.08813, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0483 - acc: 0.9797 - val_loss: 0.0881 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0492 - acc: 0.9844Epoch 00064: val_loss improved from 0.08813 to 0.08636, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0475 - acc: 0.9797 - val_loss: 0.0864 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0384 - acc: 0.9844Epoch 00065: val_loss improved from 0.08636 to 0.08489, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0468 - acc: 0.9784 - val_loss: 0.0849 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0405 - acc: 0.9844Epoch 00066: val_loss improved from 0.08489 to 0.08322, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0473 - acc: 0.9757 - val_loss: 0.0832 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0588 - acc: 0.9844Epoch 00067: val_loss improved from 0.08322 to 0.08204, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0452 - acc: 0.9824 - val_loss: 0.0820 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0330 - acc: 0.9844Epoch 00068: val_loss improved from 0.08204 to 0.08058, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0455 - acc: 0.9811 - val_loss: 0.0806 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0607 - acc: 0.9688Epoch 00069: val_loss improved from 0.08058 to 0.07941, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0462 - acc: 0.9811 - val_loss: 0.0794 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0427 - acc: 0.9688Epoch 00070: val_loss improved from 0.07941 to 0.07815, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0444 - acc: 0.9811 - val_loss: 0.0782 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0389 - acc: 0.9844Epoch 00071: val_loss improved from 0.07815 to 0.07688, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0446 - acc: 0.9811 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0395 - acc: 0.9688Epoch 00072: val_loss improved from 0.07688 to 0.07571, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0440 - acc: 0.9797 - val_loss: 0.0757 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0238 - acc: 0.9844Epoch 00073: val_loss improved from 0.07571 to 0.07438, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0446 - acc: 0.9757 - val_loss: 0.0744 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0421 - acc: 0.9844Epoch 00074: val_loss improved from 0.07438 to 0.07313, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0439 - acc: 0.9770 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0428 - acc: 0.9797Epoch 00075: val_loss improved from 0.07313 to 0.07205, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0429 - acc: 0.9811 - val_loss: 0.0720 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0443 - acc: 0.9844Epoch 00076: val_loss improved from 0.07205 to 0.07091, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0427 - acc: 0.9770 - val_loss: 0.0709 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0775 - acc: 0.9609Epoch 00077: val_loss improved from 0.07091 to 0.06985, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0426 - acc: 0.9784 - val_loss: 0.0699 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0468 - acc: 0.9688Epoch 00078: val_loss improved from 0.06985 to 0.06880, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0435 - acc: 0.9770 - val_loss: 0.0688 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0470 - acc: 0.9688Epoch 00079: val_loss improved from 0.06880 to 0.06781, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0414 - acc: 0.9811 - val_loss: 0.0678 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0482 - acc: 0.9766Epoch 00080: val_loss improved from 0.06781 to 0.06677, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0416 - acc: 0.9797 - val_loss: 0.0668 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0256 - acc: 0.9844Epoch 00081: val_loss improved from 0.06677 to 0.06583, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0416 - acc: 0.9797 - val_loss: 0.0658 - val_acc: 1.0000\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0346 - acc: 0.9844Epoch 00082: val_loss improved from 0.06583 to 0.06490, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0411 - acc: 0.9784 - val_loss: 0.0649 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0437 - acc: 0.9781Epoch 00083: val_loss improved from 0.06490 to 0.06396, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0401 - acc: 0.9811 - val_loss: 0.0640 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0427 - acc: 0.9766Epoch 00084: val_loss improved from 0.06396 to 0.06314, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0416 - acc: 0.9797 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0421 - acc: 0.9812Epoch 00085: val_loss improved from 0.06314 to 0.06209, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0397 - acc: 0.9811 - val_loss: 0.0621 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0713 - acc: 0.9688Epoch 00086: val_loss improved from 0.06209 to 0.06115, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0393 - acc: 0.9811 - val_loss: 0.0611 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0270 - acc: 0.9844Epoch 00087: val_loss improved from 0.06115 to 0.06037, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0405 - acc: 0.9784 - val_loss: 0.0604 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0174 - acc: 1.0000Epoch 00088: val_loss improved from 0.06037 to 0.05946, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0391 - acc: 0.9824 - val_loss: 0.0595 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0233 - acc: 0.9844Epoch 00089: val_loss improved from 0.05946 to 0.05868, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0391 - acc: 0.9797 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0748 - acc: 0.9531Epoch 00090: val_loss improved from 0.05868 to 0.05787, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0387 - acc: 0.9797 - val_loss: 0.0579 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0273 - acc: 0.9844Epoch 00091: val_loss improved from 0.05787 to 0.05708, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0403 - acc: 0.9811 - val_loss: 0.0571 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0233 - acc: 0.9844Epoch 00092: val_loss improved from 0.05708 to 0.05627, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0408 - acc: 0.9730 - val_loss: 0.0563 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0812 - acc: 0.9375Epoch 00093: val_loss improved from 0.05627 to 0.05562, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0394 - acc: 0.9811 - val_loss: 0.0556 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0551 - acc: 0.9688Epoch 00094: val_loss improved from 0.05562 to 0.05484, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0384 - acc: 0.9811 - val_loss: 0.0548 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0303 - acc: 0.9844Epoch 00095: val_loss improved from 0.05484 to 0.05421, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0385 - acc: 0.9811 - val_loss: 0.0542 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0358 - acc: 0.9844Epoch 00096: val_loss improved from 0.05421 to 0.05340, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0394 - acc: 0.9757 - val_loss: 0.0534 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0201 - acc: 1.0000Epoch 00097: val_loss improved from 0.05340 to 0.05278, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0365 - acc: 0.9811 - val_loss: 0.0528 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0047 - acc: 1.0000Epoch 00098: val_loss improved from 0.05278 to 0.05218, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0392 - acc: 0.9784 - val_loss: 0.0522 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0204 - acc: 1.0000Epoch 00099: val_loss improved from 0.05218 to 0.05138, saving model to Model1.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0362 - acc: 0.9811 - val_loss: 0.0514 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "file_path = \"Model1.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,\n",
    "                              save_best_only=True, mode=\"min\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "callbacks_list = [check_point, early_stop]\n",
    "\n",
    "# train\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features+1, embed_size,\n",
    "                    weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# model training\n",
    "history = model.fit(X_train, y_ohe, batch_size=32, epochs=100, callbacks=callbacks_list,\n",
    "                    validation_split=0.1, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 20, 300)           153000    \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 20, 64)            57664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 10, 64)            12352     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 225,228\n",
      "Trainable params: 72,228\n",
      "Non-trainable params: 153,000\n",
      "_________________________________________________________________\n",
      "Train on 185 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.6513 - acc: 0.6562Epoch 00000: val_loss improved from inf to 0.58364, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.6477 - acc: 0.6676 - val_loss: 0.5836 - val_acc: 0.7500\n",
      "Epoch 2/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.5631 - acc: 0.7656Epoch 00001: val_loss improved from 0.58364 to 0.54994, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.5583 - acc: 0.7514 - val_loss: 0.5499 - val_acc: 0.7500\n",
      "Epoch 3/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.5033 - acc: 0.7641Epoch 00002: val_loss improved from 0.54994 to 0.54903, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.5019 - acc: 0.7635 - val_loss: 0.5490 - val_acc: 0.7500\n",
      "Epoch 4/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.4611 - acc: 0.7812Epoch 00003: val_loss improved from 0.54903 to 0.53356, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.4624 - acc: 0.7811 - val_loss: 0.5336 - val_acc: 0.7500\n",
      "Epoch 5/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.4184 - acc: 0.8187Epoch 00004: val_loss improved from 0.53356 to 0.52611, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.4093 - acc: 0.8230 - val_loss: 0.5261 - val_acc: 0.7500\n",
      "Epoch 6/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.3850 - acc: 0.8266Epoch 00005: val_loss improved from 0.52611 to 0.51267, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3921 - acc: 0.8284 - val_loss: 0.5127 - val_acc: 0.7619\n",
      "Epoch 7/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.3593 - acc: 0.8594Epoch 00006: val_loss improved from 0.51267 to 0.47854, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3602 - acc: 0.8473 - val_loss: 0.4785 - val_acc: 0.8095\n",
      "Epoch 8/100\n",
      "128/185 [===================>..........] - ETA: 0s - loss: 0.3069 - acc: 0.8770Epoch 00007: val_loss improved from 0.47854 to 0.44345, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3235 - acc: 0.8689 - val_loss: 0.4434 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2904 - acc: 0.8891Epoch 00008: val_loss improved from 0.44345 to 0.42512, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2825 - acc: 0.8932 - val_loss: 0.4251 - val_acc: 0.8452\n",
      "Epoch 10/100\n",
      "128/185 [===================>..........] - ETA: 0s - loss: 0.2628 - acc: 0.9043Epoch 00009: val_loss improved from 0.42512 to 0.39898, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2634 - acc: 0.8986 - val_loss: 0.3990 - val_acc: 0.8452\n",
      "Epoch 11/100\n",
      "128/185 [===================>..........] - ETA: 0s - loss: 0.2556 - acc: 0.9102Epoch 00010: val_loss improved from 0.39898 to 0.37709, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2488 - acc: 0.9027 - val_loss: 0.3771 - val_acc: 0.8452\n",
      "Epoch 12/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2116 - acc: 0.9141Epoch 00011: val_loss improved from 0.37709 to 0.34055, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2070 - acc: 0.9189 - val_loss: 0.3406 - val_acc: 0.8571\n",
      "Epoch 13/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1983 - acc: 0.9250Epoch 00012: val_loss improved from 0.34055 to 0.30489, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1954 - acc: 0.9270 - val_loss: 0.3049 - val_acc: 0.9048\n",
      "Epoch 14/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1871 - acc: 0.9375Epoch 00013: val_loss improved from 0.30489 to 0.26455, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1841 - acc: 0.9419 - val_loss: 0.2646 - val_acc: 0.9524\n",
      "Epoch 15/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1645 - acc: 0.9516Epoch 00014: val_loss improved from 0.26455 to 0.25222, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1668 - acc: 0.9527 - val_loss: 0.2522 - val_acc: 0.9643\n",
      "Epoch 16/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1581 - acc: 0.9625Epoch 00015: val_loss improved from 0.25222 to 0.24055, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1727 - acc: 0.9527 - val_loss: 0.2405 - val_acc: 0.9643\n",
      "Epoch 17/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1503 - acc: 0.9484Epoch 00016: val_loss improved from 0.24055 to 0.21288, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1532 - acc: 0.9486 - val_loss: 0.2129 - val_acc: 0.9643\n",
      "Epoch 18/100\n",
      "128/185 [===================>..........] - ETA: 0s - loss: 0.1050 - acc: 0.9785Epoch 00017: val_loss improved from 0.21288 to 0.19175, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1253 - acc: 0.9703 - val_loss: 0.1917 - val_acc: 0.9762\n",
      "Epoch 19/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1650 - acc: 0.9375Epoch 00018: val_loss improved from 0.19175 to 0.18507, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1238 - acc: 0.9568 - val_loss: 0.1851 - val_acc: 0.9762\n",
      "Epoch 20/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1211 - acc: 0.9688Epoch 00019: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1290 - acc: 0.9568 - val_loss: 0.1861 - val_acc: 0.9643\n",
      "Epoch 21/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1321 - acc: 0.9531Epoch 00020: val_loss improved from 0.18507 to 0.18465, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1060 - acc: 0.9743 - val_loss: 0.1846 - val_acc: 0.9643\n",
      "Epoch 22/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0914 - acc: 0.9766Epoch 00021: val_loss improved from 0.18465 to 0.17626, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0973 - acc: 0.9757 - val_loss: 0.1763 - val_acc: 0.9643\n",
      "Epoch 23/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1079 - acc: 0.9609Epoch 00022: val_loss improved from 0.17626 to 0.16096, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1078 - acc: 0.9649 - val_loss: 0.1610 - val_acc: 0.9881\n",
      "Epoch 24/100\n",
      "128/185 [===================>..........] - ETA: 0s - loss: 0.1038 - acc: 0.9668Epoch 00023: val_loss improved from 0.16096 to 0.14654, saving model to Model2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 0s - loss: 0.1043 - acc: 0.9716 - val_loss: 0.1465 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1074 - acc: 0.9688Epoch 00024: val_loss improved from 0.14654 to 0.13477, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0913 - acc: 0.9716 - val_loss: 0.1348 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0894 - acc: 0.9781Epoch 00025: val_loss improved from 0.13477 to 0.12422, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0854 - acc: 0.9770 - val_loss: 0.1242 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0558 - acc: 0.9922Epoch 00026: val_loss improved from 0.12422 to 0.11196, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0844 - acc: 0.9811 - val_loss: 0.1120 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0707 - acc: 0.9844Epoch 00027: val_loss improved from 0.11196 to 0.10761, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0901 - acc: 0.9689 - val_loss: 0.1076 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "128/185 [===================>..........] - ETA: 0s - loss: 0.0760 - acc: 0.9766Epoch 00028: val_loss improved from 0.10761 to 0.10198, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0828 - acc: 0.9730 - val_loss: 0.1020 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0910 - acc: 0.9609Epoch 00029: val_loss improved from 0.10198 to 0.09894, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0998 - acc: 0.9649 - val_loss: 0.0989 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0893 - acc: 0.9766Epoch 00030: val_loss improved from 0.09894 to 0.09344, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0907 - acc: 0.9743 - val_loss: 0.0934 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0763 - acc: 0.9766Epoch 00031: val_loss improved from 0.09344 to 0.08796, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0736 - acc: 0.9784 - val_loss: 0.0880 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0787 - acc: 0.9781Epoch 00032: val_loss improved from 0.08796 to 0.08121, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0831 - acc: 0.9743 - val_loss: 0.0812 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0703 - acc: 0.9766Epoch 00033: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0676 - acc: 0.9770 - val_loss: 0.0821 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0599 - acc: 0.9828Epoch 00034: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0605 - acc: 0.9824 - val_loss: 0.0843 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0549 - acc: 0.9812Epoch 00035: val_loss improved from 0.08121 to 0.07957, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0610 - acc: 0.9784 - val_loss: 0.0796 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1146 - acc: 0.9453Epoch 00036: val_loss improved from 0.07957 to 0.07095, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0657 - acc: 0.9784 - val_loss: 0.0710 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0806 - acc: 0.9609Epoch 00037: val_loss improved from 0.07095 to 0.06536, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0655 - acc: 0.9770 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0585 - acc: 0.9828Epoch 00038: val_loss improved from 0.06536 to 0.06291, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0678 - acc: 0.9797 - val_loss: 0.0629 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0820 - acc: 0.9844Epoch 00039: val_loss improved from 0.06291 to 0.05880, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0790 - acc: 0.9770 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0813 - acc: 0.9766Epoch 00040: val_loss improved from 0.05880 to 0.05566, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0541 - acc: 0.9811 - val_loss: 0.0557 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0563 - acc: 0.9844Epoch 00041: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0615 - acc: 0.9838 - val_loss: 0.0576 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0612 - acc: 0.9766Epoch 00042: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0584 - acc: 0.9784 - val_loss: 0.0593 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0427 - acc: 0.9844Epoch 00043: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0578 - acc: 0.9784 - val_loss: 0.0583 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0541 - acc: 0.9812Epoch 00044: val_loss improved from 0.05566 to 0.05424, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0567 - acc: 0.9811 - val_loss: 0.0542 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0466 - acc: 0.9812Epoch 00045: val_loss improved from 0.05424 to 0.05205, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0520 - acc: 0.9770 - val_loss: 0.0521 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0515 - acc: 0.9859Epoch 00046: val_loss improved from 0.05205 to 0.04906, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0511 - acc: 0.9865 - val_loss: 0.0491 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.1032 - acc: 0.9531Epoch 00047: val_loss improved from 0.04906 to 0.04542, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0703 - acc: 0.9716 - val_loss: 0.0454 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0466 - acc: 0.9766Epoch 00048: val_loss improved from 0.04542 to 0.04532, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0460 - acc: 0.9838 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0631 - acc: 0.9766Epoch 00049: val_loss improved from 0.04532 to 0.04486, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0575 - acc: 0.9824 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0381 - acc: 0.9922Epoch 00050: val_loss improved from 0.04486 to 0.04246, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0521 - acc: 0.9838 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0454 - acc: 0.9766Epoch 00051: val_loss improved from 0.04246 to 0.03943, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0466 - acc: 0.9824 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0380 - acc: 1.0000Epoch 00052: val_loss improved from 0.03943 to 0.03599, saving model to Model2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 0s - loss: 0.0626 - acc: 0.9838 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0612 - acc: 0.9766Epoch 00053: val_loss improved from 0.03599 to 0.03459, saving model to Model2.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.0550 - acc: 0.9797 - val_loss: 0.0346 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0583 - acc: 0.9734Epoch 00054: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0540 - acc: 0.9770 - val_loss: 0.0348 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0213 - acc: 1.0000Epoch 00055: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0454 - acc: 0.9838 - val_loss: 0.0366 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0222 - acc: 1.0000Epoch 00056: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0571 - acc: 0.9757 - val_loss: 0.0393 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      " 32/185 [====>.........................] - ETA: 0s - loss: 0.0639 - acc: 0.9688Epoch 00057: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0553 - acc: 0.9770 - val_loss: 0.0422 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "file_path = \"Model2.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,\n",
    "                              save_best_only=True, mode=\"min\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "callbacks_list = [check_point, early_stop]\n",
    "\n",
    "# train\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features+1, embed_size,\n",
    "                    weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Dense(4, activation='sigmoid'))  # multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# model training\n",
    "history = model.fit(X_train, y_ohe, batch_size=32, epochs=100, callbacks=callbacks_list,\n",
    "                    validation_split=0.1, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Model3.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features+1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(4, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 32, epochs = 100, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)         (None, 20, 300)       153000      input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDrop (None, 20, 300)       0           embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 20, 128)       140160      spatial_dropout1d_4[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)               (None, 18, 32)        12320       bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional)  (None, 18, 128)       49664       conv1d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)               (None, 19, 32)        8224        bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)               (None, 16, 32)        12320       bidirectional_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)               (None, 17, 32)        8224        bidirectional_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glob (None, 32)            0           conv1d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (GlobalM (None, 32)            0           conv1d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Glo (None, 32)            0           conv1d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (GlobalM (None, 32)            0           conv1d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Glo (None, 32)            0           conv1d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (GlobalM (None, 32)            0           conv1d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Glo (None, 32)            0           conv1d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (GlobalM (None, 32)            0           conv1d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 256)           0           global_average_pooling1d_9[0][0] \n",
      "                                                                   global_max_pooling1d_25[0][0]    \n",
      "                                                                   global_average_pooling1d_10[0][0]\n",
      "                                                                   global_max_pooling1d_26[0][0]    \n",
      "                                                                   global_average_pooling1d_11[0][0]\n",
      "                                                                   global_max_pooling1d_27[0][0]    \n",
      "                                                                   global_average_pooling1d_12[0][0]\n",
      "                                                                   global_max_pooling1d_28[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 256)           1024        concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (None, 128)           32896       batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 128)           0           dense_35[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 128)           512         dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_36 (Dense)                 (None, 64)            8256        batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 64)            0           dense_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_37 (Dense)                 (None, 4)             260         dropout_18[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 426,860\n",
      "Trainable params: 273,092\n",
      "Non-trainable params: 153,768\n",
      "____________________________________________________________________________________________________\n",
      "Train on 185 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.7601 - acc: 0.5344Epoch 00000: val_loss improved from inf to 0.63515, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 5s - loss: 0.7490 - acc: 0.5459 - val_loss: 0.6352 - val_acc: 0.8929\n",
      "Epoch 2/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.5923 - acc: 0.6969Epoch 00001: val_loss improved from 0.63515 to 0.60886, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.5695 - acc: 0.7149 - val_loss: 0.6089 - val_acc: 0.8929\n",
      "Epoch 3/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.4504 - acc: 0.8109Epoch 00002: val_loss improved from 0.60886 to 0.59054, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.4407 - acc: 0.8162 - val_loss: 0.5905 - val_acc: 0.8810\n",
      "Epoch 4/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.3654 - acc: 0.8594Epoch 00003: val_loss improved from 0.59054 to 0.57588, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3604 - acc: 0.8581 - val_loss: 0.5759 - val_acc: 0.8690\n",
      "Epoch 5/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.3161 - acc: 0.8797Epoch 00004: val_loss improved from 0.57588 to 0.55873, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.3019 - acc: 0.8905 - val_loss: 0.5587 - val_acc: 0.8690\n",
      "Epoch 6/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2654 - acc: 0.9062Epoch 00005: val_loss improved from 0.55873 to 0.55813, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.2567 - acc: 0.9122 - val_loss: 0.5581 - val_acc: 0.8214\n",
      "Epoch 7/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2457 - acc: 0.9156Epoch 00006: val_loss improved from 0.55813 to 0.55707, saving model to Model3.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 0s - loss: 0.2465 - acc: 0.9176 - val_loss: 0.5571 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2139 - acc: 0.9156Epoch 00007: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.2152 - acc: 0.9176 - val_loss: 0.5687 - val_acc: 0.8095\n",
      "Epoch 9/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.2091 - acc: 0.9281Epoch 00008: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.2103 - acc: 0.9270 - val_loss: 0.5658 - val_acc: 0.8095\n",
      "Epoch 10/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1854 - acc: 0.9437Epoch 00009: val_loss improved from 0.55707 to 0.54081, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1908 - acc: 0.9365 - val_loss: 0.5408 - val_acc: 0.8214\n",
      "Epoch 11/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1642 - acc: 0.9437Epoch 00010: val_loss improved from 0.54081 to 0.51900, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1803 - acc: 0.9338 - val_loss: 0.5190 - val_acc: 0.8333\n",
      "Epoch 12/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1482 - acc: 0.9437Epoch 00011: val_loss improved from 0.51900 to 0.51494, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1590 - acc: 0.9392 - val_loss: 0.5149 - val_acc: 0.8214\n",
      "Epoch 13/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1440 - acc: 0.9406Epoch 00012: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1492 - acc: 0.9378 - val_loss: 0.5440 - val_acc: 0.8095\n",
      "Epoch 14/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1794 - acc: 0.9266Epoch 00013: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1776 - acc: 0.9284 - val_loss: 0.5634 - val_acc: 0.7857\n",
      "Epoch 15/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1469 - acc: 0.9469Epoch 00014: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1517 - acc: 0.9446 - val_loss: 0.5517 - val_acc: 0.7976\n",
      "Epoch 16/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1421 - acc: 0.9375Epoch 00015: val_loss improved from 0.51494 to 0.51248, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1400 - acc: 0.9378 - val_loss: 0.5125 - val_acc: 0.8095\n",
      "Epoch 17/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1292 - acc: 0.9578Epoch 00016: val_loss improved from 0.51248 to 0.49059, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1350 - acc: 0.9541 - val_loss: 0.4906 - val_acc: 0.8095\n",
      "Epoch 18/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1257 - acc: 0.9547Epoch 00017: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1257 - acc: 0.9554 - val_loss: 0.5224 - val_acc: 0.8095\n",
      "Epoch 19/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1323 - acc: 0.9437Epoch 00018: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1416 - acc: 0.9378 - val_loss: 0.5579 - val_acc: 0.7500\n",
      "Epoch 20/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1168 - acc: 0.9531Epoch 00019: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1104 - acc: 0.9554 - val_loss: 0.5314 - val_acc: 0.7738\n",
      "Epoch 21/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1387 - acc: 0.9547Epoch 00020: val_loss improved from 0.49059 to 0.47065, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1321 - acc: 0.9568 - val_loss: 0.4707 - val_acc: 0.8214\n",
      "Epoch 22/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1362 - acc: 0.9406Epoch 00021: val_loss improved from 0.47065 to 0.46989, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1334 - acc: 0.9446 - val_loss: 0.4699 - val_acc: 0.8214\n",
      "Epoch 23/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0960 - acc: 0.9703Epoch 00022: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1037 - acc: 0.9622 - val_loss: 0.4936 - val_acc: 0.7976\n",
      "Epoch 24/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1080 - acc: 0.9531Epoch 00023: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1101 - acc: 0.9514 - val_loss: 0.4719 - val_acc: 0.7976\n",
      "Epoch 25/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1152 - acc: 0.9531Epoch 00024: val_loss improved from 0.46989 to 0.41789, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1154 - acc: 0.9527 - val_loss: 0.4179 - val_acc: 0.8333\n",
      "Epoch 26/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1176 - acc: 0.9578Epoch 00025: val_loss improved from 0.41789 to 0.38132, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1146 - acc: 0.9581 - val_loss: 0.3813 - val_acc: 0.8333\n",
      "Epoch 27/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1027 - acc: 0.9625Epoch 00026: val_loss improved from 0.38132 to 0.35676, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1024 - acc: 0.9622 - val_loss: 0.3568 - val_acc: 0.8333\n",
      "Epoch 28/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1131 - acc: 0.9516Epoch 00027: val_loss improved from 0.35676 to 0.33785, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1128 - acc: 0.9486 - val_loss: 0.3379 - val_acc: 0.8333\n",
      "Epoch 29/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1202 - acc: 0.9500Epoch 00028: val_loss improved from 0.33785 to 0.30941, saving model to Model3.hdf5\n",
      "185/185 [==============================] - 0s - loss: 0.1180 - acc: 0.9486 - val_loss: 0.3094 - val_acc: 0.8452\n",
      "Epoch 30/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1686 - acc: 0.9219Epoch 00029: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1595 - acc: 0.9270 - val_loss: 0.3476 - val_acc: 0.8333\n",
      "Epoch 31/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1017 - acc: 0.9641Epoch 00030: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1128 - acc: 0.9554 - val_loss: 0.3992 - val_acc: 0.8095\n",
      "Epoch 32/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.0958 - acc: 0.9641Epoch 00031: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.0913 - acc: 0.9662 - val_loss: 0.4248 - val_acc: 0.7976\n",
      "Epoch 33/100\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 0.1037 - acc: 0.9625Epoch 00032: val_loss did not improve\n",
      "185/185 [==============================] - 0s - loss: 0.1021 - acc: 0.9608 - val_loss: 0.4218 - val_acc: 0.8095\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/206 [===>..........................] - ETA: 8s[0.037539691130802466, 0.98300970873786409]\n"
     ]
    }
   ],
   "source": [
    "model1_file_path = 'Model1.hdf5'\n",
    "model1 = load_model(model1_file_path)\n",
    "score1 = model1.evaluate(X_train, y_ohe, verbose = 1)\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/206 [===>..........................] - ETA: 9s[0.038978623533711849, 0.98300970873786409]\n"
     ]
    }
   ],
   "source": [
    "model2_file_path = 'Model2.hdf5'\n",
    "model2 = load_model(model2_file_path)\n",
    "score = model2.evaluate(X_train, y_ohe, verbose = 1)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/206 [==========================>...] - ETA: 0s[0.1797583340441139, 0.94417475843892518]\n"
     ]
    }
   ],
   "source": [
    "model3_file_path = 'Model3.hdf5'\n",
    "model3 = load_model(model3_file_path)\n",
    "score3 = model3.evaluate(X_train, y_ohe, verbose = 1)\n",
    "print(score3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
